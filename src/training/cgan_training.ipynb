{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os, sys, json\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.models import load_model\n",
    "from PIL import Image"
   ],
   "id": "addf5d5e3dc7645a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('..','data_processing')))\n",
    "sys.path.append(os.path.abspath(os.path.join('..','models')))"
   ],
   "id": "b648461341767090",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from cgan_preprocessing import dataset\n",
   "id": "314c6910df624c1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from cgan import create_cgan\n",
    "from cgan2 import create_cgan2"
   ],
   "id": "e770e1a02f1e0012",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from cgan3 import create_cgan3",
   "id": "bd991e81313e14b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## cGan (64x64)",
   "id": "3e67cf020ab06669"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_cgan(\n",
    "        generator,\n",
    "        discriminator,\n",
    "        cgan,\n",
    "        dataset,\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        latent_dim=100,\n",
    "        save_dir='cgan_training',\n",
    "        starting_epoch=0,\n",
    "        checkpoint_interval=10,\n",
    "        sample_interval=5\n",
    "):\n",
    "    save_dir = Path(save_dir)\n",
    "    models_dir = save_dir / 'models'\n",
    "    samples_dir = save_dir / 'samples'\n",
    "\n",
    "    # Create directories if they dont exist\n",
    "    for dir_path in [save_dir, models_dir, samples_dir]:\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load existing history if resuming training\n",
    "    history_file = save_dir / 'training_history.json'\n",
    "    if starting_epoch > 0 and history_file.exists():\n",
    "        with open(history_file, 'r') as f:\n",
    "            history = json.load(f)\n",
    "        print(f\"Loaded existing training history from epoch {starting_epoch}\")\n",
    "    else:\n",
    "        history = {\n",
    "            'training_config': {\n",
    "                'batch_size': int(batch_size),\n",
    "                'latent_dim': int(latent_dim),\n",
    "                'starting_epoch': int(starting_epoch),\n",
    "                'total_epochs': int(epochs),\n",
    "                'checkpoint_interval': int(checkpoint_interval),\n",
    "                'sample_interval': int(sample_interval),\n",
    "                'start_time': datetime.now().isoformat()\n",
    "            },\n",
    "            'epochs': []\n",
    "        }\n",
    "\n",
    "    def save_images(epoch):\n",
    "        try:\n",
    "            print(f\"Attempting to generate images for epoch {epoch}\")\n",
    "            print(f\"Saving to directory: {samples_dir}\")\n",
    "\n",
    "            # Generate images for different conditions\n",
    "            conditions = [\n",
    "                [0, 0],  # Female, Not Smiling\n",
    "                [0, 1],  # Female, Smiling\n",
    "                [1, 0],  # Male, Not Smiling\n",
    "                [1, 1]   # Male, Smiling\n",
    "            ]\n",
    "            rows = 4  # One row for each condition\n",
    "            cols = 4  # Number of samples per condition\n",
    "\n",
    "            plt.figure(figsize=(12, 12))\n",
    "\n",
    "            for condition_idx, condition in enumerate(conditions):\n",
    "                noise = tf.random.normal([cols, latent_dim])\n",
    "                condition_batch = tf.tile(tf.constant([condition]), [cols, 1])\n",
    "\n",
    "                generated = generator([noise, condition_batch], training=False)\n",
    "                generated = (generated + 1) / 2.0  # Normalize to [0,1]\n",
    "\n",
    "                condition_label = f\"{'Male' if condition[0] else 'Female'}, {'Smiling' if condition[1] else 'Not Smiling'}\"\n",
    "\n",
    "                for col in range(cols):\n",
    "                    plt.subplot(rows, cols, condition_idx * cols + col + 1)\n",
    "                    if col == 0:\n",
    "                        plt.ylabel(condition_label, fontsize=8)\n",
    "                    plt.imshow(generated[col])\n",
    "                    plt.axis('off')\n",
    "\n",
    "            save_path = samples_dir / f'epoch_{epoch}.png'\n",
    "            print(f\"Attempting to save figure to: {save_path}\")\n",
    "            plt.savefig(save_path)\n",
    "            print(f\"Successfully saved figure to: {save_path}\")\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "            if save_path.exists():\n",
    "                print(f\"Verified: File exists at {save_path}\")\n",
    "                print(f\"File size: {save_path.stat().st_size} bytes\")\n",
    "            else:\n",
    "                print(f\"Warning: File was not found at {save_path} after saving\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in save_images: {str(e)}\")\n",
    "            print(f\"Error type: {type(e)}\")\n",
    "            import traceback\n",
    "            print(f\"Traceback: {traceback.format_exc()}\")\n",
    "\n",
    "    def save_checkpoint(epoch, d_losses, g_losses):\n",
    "        epoch_stats = {\n",
    "            'epoch_number': int(epoch),\n",
    "            'epoch_completed': datetime.now().isoformat(),\n",
    "            'mean_d_loss': float(np.mean(d_losses)),\n",
    "            'mean_g_loss': float(np.mean(g_losses)),\n",
    "            'std_d_loss': float(np.std(d_losses)),\n",
    "            'std_g_loss': float(np.std(g_losses))\n",
    "        }\n",
    "        history['epochs'].append(epoch_stats)\n",
    "\n",
    "        checkpoint_dir = models_dir / f'checkpoint_epoch_{epoch}'\n",
    "        checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        generator.save(checkpoint_dir / 'generator.h5', include_optimizer=True)\n",
    "        discriminator.save(checkpoint_dir / 'discriminator.h5', include_optimizer=True)\n",
    "\n",
    "        with open(history_file, 'w') as f:\n",
    "            json.dump(history, f, indent=4)\n",
    "\n",
    "    print(f\"Starting/Resuming training from epoch {starting_epoch + 1}...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(starting_epoch, starting_epoch + epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}\")\n",
    "        epoch_d_losses = []\n",
    "        epoch_g_losses = []\n",
    "        batch_count = 0\n",
    "\n",
    "        for batch_images, batch_labels in dataset:\n",
    "            batch_size = tf.shape(batch_images)[0]\n",
    "\n",
    "            # Train discriminator\n",
    "            noise = tf.random.normal([batch_size, latent_dim])\n",
    "            generated_images = generator([noise, batch_labels], training=True)\n",
    "\n",
    "            # Add label noise for better training stability\n",
    "            real_labels = tf.random.uniform([batch_size, 1], 0.8, 1.0)\n",
    "            fake_labels = tf.random.uniform([batch_size, 1], 0.0, 0.2)\n",
    "\n",
    "            # Train discriminator on real images\n",
    "            d_loss_real = discriminator.train_on_batch(\n",
    "                [batch_images, batch_labels],\n",
    "                real_labels\n",
    "            )\n",
    "            loss_real = d_loss_real[0]\n",
    "\n",
    "            # Train discriminator on fake images\n",
    "            d_loss_fake = discriminator.train_on_batch(\n",
    "                [generated_images, batch_labels],\n",
    "                fake_labels\n",
    "            )\n",
    "            loss_fake = d_loss_fake[0]\n",
    "\n",
    "            d_loss = 0.5 * (loss_real + loss_fake)\n",
    "\n",
    "            # Train generator\n",
    "            noise = tf.random.normal([batch_size * 2, latent_dim])\n",
    "            # Double the labels batch for generator training\n",
    "            g_labels = tf.tile(batch_labels, [2, 1])\n",
    "            g_loss = cgan.train_on_batch(\n",
    "                [noise, g_labels],\n",
    "                tf.ones([batch_size * 2, 1])\n",
    "            )\n",
    "\n",
    "            epoch_d_losses.append(float(d_loss))\n",
    "            epoch_g_losses.append(float(g_loss))\n",
    "\n",
    "            if batch_count % 50 == 0:\n",
    "                print(f\"Batch {batch_count}: d_loss={d_loss:.4f}, g_loss={g_loss:.4f}\")\n",
    "            batch_count += 1\n",
    "\n",
    "        absolute_epoch = epoch + 1\n",
    "\n",
    "        # Save samples every sample_interval epochs\n",
    "        if absolute_epoch % sample_interval == 0:\n",
    "            print(f\"Generating sample images at epoch {absolute_epoch}\")\n",
    "            save_images(absolute_epoch)\n",
    "\n",
    "        # Save checkpoint every checkpoint_interval epochs\n",
    "        if absolute_epoch % checkpoint_interval == 0:\n",
    "            print(f\"Saving checkpoint at epoch {absolute_epoch}\")\n",
    "            save_checkpoint(absolute_epoch, epoch_d_losses, epoch_g_losses)\n",
    "\n",
    "        # Always update history\n",
    "        with open(history_file, 'w') as f:\n",
    "            json.dump(history, f, indent=4)\n",
    "\n",
    "    # Save final checkpoint\n",
    "    save_checkpoint(starting_epoch + epochs, epoch_d_losses, epoch_g_losses)\n",
    "    print(f\"\\nTraining completed in {time.time() - start_time:.1f} seconds\")\n",
    "    return history"
   ],
   "id": "59947e29eb765c6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def load_gan_checkpoint(checkpoint_dir, latent_dim=100):\n",
    "\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "\n",
    "    # Load full models\n",
    "    generator = tf.keras.models.load_model(checkpoint_dir / 'generator.h5', compile=False)\n",
    "    discriminator = tf.keras.models.load_model(checkpoint_dir / 'discriminator.h5', compile=False)\n",
    "\n",
    "    optimizer_g = tf.keras.optimizers.Adam(learning_rate=2e-9, beta_1=0.5, beta_2=0.999)\n",
    "    optimizer_d = tf.keras.optimizers.Adam(learning_rate=2e-9, beta_1=0.5, beta_2=0.999)\n",
    "\n",
    "    # Recreate GAN\n",
    "    discriminator.trainable = False\n",
    "\n",
    "    noise_input = layers.Input(shape=(latent_dim,))\n",
    "    label_input = layers.Input(shape=(2,))\n",
    "\n",
    "    generated_images = generator([noise_input, label_input])\n",
    "    validity = discriminator([generated_images, label_input])\n",
    "\n",
    "    cgan = models.Model([noise_input, label_input], validity)\n",
    "\n",
    "    # Compile with same settings\n",
    "    discriminator.compile(\n",
    "        optimizer=optimizer_d,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False, label_smoothing=0.1),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    cgan.compile(\n",
    "        optimizer=optimizer_g,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    )\n",
    "\n",
    "    return generator, discriminator, cgan"
   ],
   "id": "5df4fb555b240df1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # To continue training:\n",
    "# Load the last checkpoint\n",
    "last_epoch = 150  # or whatever your last epoch was\n",
    "checkpoint_path = f'../results/cgan2/models/checkpoint_epoch_150'\n",
    "generator, discriminator, cgan = load_gan_checkpoint(checkpoint_path)\n",
    "\n",
    "# Continue training\n",
    "his = train_cgan(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    cgan=cgan,\n",
    "    dataset=dataset,\n",
    "    epochs=50,  # additional epochs\n",
    "    save_dir='../results/cgan2',\n",
    "    starting_epoch=last_epoch  # continue from last epoch\n",
    ")"
   ],
   "id": "4fd2da588a172c94",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "time.sleep(60*45)",
   "id": "ce4347ae21d112ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # To continue training:\n",
    "# Load the last checkpoint\n",
    "last_epoch = 200  # or whatever your last epoch was\n",
    "checkpoint_path = f'../results/cgan2/models/checkpoint_epoch_200'\n",
    "generator, discriminator, cgan = load_gan_checkpoint(checkpoint_path)\n",
    "\n",
    "# Continue training\n",
    "his2 = train_cgan(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    cgan=cgan,\n",
    "    dataset=dataset,\n",
    "    epochs=50,  # additional epochs\n",
    "    save_dir='../results/cgan2',\n",
    "    starting_epoch=last_epoch  # continue from last epoch\n",
    ")"
   ],
   "id": "77a0cf207540131a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "time.sleep(60*45)",
   "id": "fda13f94e148a8ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    # To continue training:\n",
    "# Load the last checkpoint\n",
    "last_epoch = 250  # or whatever your last epoch was\n",
    "checkpoint_path = f'../results/cgan2/models/checkpoint_epoch_250'\n",
    "generator, discriminator, cgan = load_gan_checkpoint(checkpoint_path)\n",
    "\n",
    "# Continue training\n",
    "his7v2 = train_cgan(\n",
    "    generator=generator,\n",
    "    discriminator=discriminator,\n",
    "    cgan=cgan,\n",
    "    dataset=dataset,\n",
    "    epochs=50,  # additional epochs\n",
    "    save_dir='../results/cgan2',\n",
    "    starting_epoch=last_epoch  # continue from last epoch\n",
    ")"
   ],
   "id": "819633db72ef49ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cgan 2 ",
   "id": "625554b0a298320e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create your models\n",
    "generator, discriminator, cgan = create_cgan2()\n",
    "\n",
    "# Train the model\n",
    "history3 = train_cgan(\n",
    "    generator,\n",
    "    discriminator,\n",
    "    cgan,\n",
    "    dataset,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    save_dir='../results/cgan2_updated'\n",
    ")"
   ],
   "id": "6b0a123999e9c0f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Cgan 3",
   "id": "1a76de649b415156"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create your models\n",
    "generator3, discriminator3, cgan3 = create_cgan3()\n",
    "\n",
    "# Train the model\n",
    "history3v2 = train_cgan(\n",
    "    generator3,\n",
    "    discriminator3,\n",
    "    cgan3,\n",
    "    dataset,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    save_dir='../results/cgan3'\n",
    ")"
   ],
   "id": "6b72c8227a76225b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "92a8ed49103ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_and_save_condition_images(generator_path, save_dir, num_images_per_condition=100, latent_dim=120):\n",
    "    # Load the trained generator model\n",
    "    generator = load_model(generator_path)\n",
    "    print(f\"Loaded generator model from {generator_path}\")\n",
    "\n",
    "    # Define the conditions\n",
    "    conditions = [\n",
    "        {'label': 'Female_Not_Smiling', 'attributes': [0, 0]},\n",
    "        {'label': 'Female_Smiling', 'attributes': [0, 1]},\n",
    "        {'label': 'Male_Not_Smiling', 'attributes': [1, 0]},\n",
    "        {'label': 'Male_Smiling', 'attributes': [1, 1]}\n",
    "    ]\n",
    "\n",
    "    # Ensure the base save directory exists\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Images will be saved to {save_dir.resolve()}\")\n",
    "\n",
    "    def save_image(tensor, condition_label, sample_number):\n",
    "        img = (tensor + 1.0) * 127.5  \n",
    "        img = img.numpy().astype('uint8') \n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        # Get the current date \n",
    "        current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")[:-3] \n",
    "\n",
    "        # Create a unique filename\n",
    "        img_filename = save_dir / condition_label / f\"image_{sample_number:05d}_{current_datetime}.png\"\n",
    "\n",
    "        # Save the image\n",
    "        img.save(img_filename)\n",
    "\n",
    "    # Iterate over each condition and generate images\n",
    "    for condition in conditions:\n",
    "        condition_label = condition['label']\n",
    "        condition_attributes = condition['attributes']\n",
    "\n",
    "        condition_dir = save_dir / condition_label\n",
    "        condition_dir.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"Saving images for condition '{condition_label}' to directory: {condition_dir.resolve()}\")\n",
    "\n",
    "        image_count = 0 \n",
    "        batch_size = 32\n",
    "        num_full_batches = num_images_per_condition // batch_size\n",
    "        remaining_images = num_images_per_condition % batch_size\n",
    "\n",
    "        # Generate and save full batches\n",
    "        for batch in range(num_full_batches):\n",
    "            # Generate noise\n",
    "            noise = tf.random.normal([batch_size, latent_dim])\n",
    "\n",
    "            # Create condition batch\n",
    "            condition_batch = tf.tile(tf.constant([condition_attributes], dtype=tf.float32), [batch_size, 1])\n",
    "\n",
    "            # Generate images\n",
    "            generated_images = generator([noise, condition_batch], training=False)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                img_tensor = generated_images[i]\n",
    "                image_count += 1\n",
    "                save_image(img_tensor, condition_label, image_count)\n",
    "\n",
    "                if image_count % 100 == 0:\n",
    "                    print(f\"{image_count} images generated and saved for condition '{condition_label}'.\")\n",
    "        if remaining_images > 0:\n",
    "            noise = tf.random.normal([remaining_images, latent_dim])\n",
    "            condition_batch = tf.tile(tf.constant([condition_attributes], dtype=tf.float32), [remaining_images, 1])\n",
    "            generated_images = generator([noise, condition_batch], training=False)\n",
    "            \n",
    "            for i in range(remaining_images):\n",
    "                img_tensor = generated_images[i]\n",
    "                image_count += 1\n",
    "                save_image(img_tensor, condition_label, image_count)\n",
    "\n",
    "                if image_count % 100 == 0:\n",
    "                    print(f\"{image_count} images generated and saved for condition '{condition_label}'.\")\n",
    "\n",
    "        print(f\"Completed generating {image_count} images for condition '{condition_label}'.\")\n",
    "\n",
    "    print(f\"\\nImage generation complete. All images saved to {save_dir.resolve()}\")\n"
   ],
   "id": "71958682cbb219fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "generate_and_save_condition_images('../results/cgan3/models/checkpoint_epoch_100/generator.h5', '../../data/cgan2_images/', num_images_per_condition=94,latent_dim=100)",
   "id": "cfa31ca67d7b364",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2a2dc3619c810444",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
