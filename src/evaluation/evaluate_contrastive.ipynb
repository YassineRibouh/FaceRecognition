{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T17:45:04.772114Z",
     "start_time": "2025-01-19T17:45:04.751697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import load_model"
   ],
   "id": "90b61664d8ed5be2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T17:39:13.556614Z",
     "start_time": "2025-01-19T17:39:13.542540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('..','data_processing')))\n",
    "sys.path.append(os.path.abspath(os.path.join('..','models')))"
   ],
   "id": "1d747ddf2e56bdf9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from contrastive_preprocessing import test_contrastive_dataset",
   "id": "99e926e93a2f1a67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluates the contrastive models on the test dataset by computing similarity scores generating ROC and PR curves and calculating performance metrics at a given threshold\n",
   "id": "6d548658488d2b8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-19T17:40:10.266047Z",
     "start_time": "2025-01-19T17:40:10.240232Z"
    }
   },
   "source": [
    "def evaluate_on_test(\n",
    "        model,\n",
    "        test_dataset,\n",
    "        threshold=0.75,\n",
    "        num_test_steps=2000\n",
    "):\n",
    "    print(\"Computing similarities from test dataset\")\n",
    "\n",
    "    all_distances = []\n",
    "    all_labels = []\n",
    "\n",
    "    for (anchor_img, comparison_img), labels in tqdm(test_dataset.take(num_test_steps)):\n",
    "        distances = model.predict([anchor_img, comparison_img], verbose=0)\n",
    "        all_distances.extend(distances.flatten())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "    all_distances = np.array(all_distances)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Convert distances to similarity scores (0-1 range)\n",
    "    similarity_scores = 1 / (1 + all_distances)\n",
    "\n",
    "    # Compute ROC and PR curves using similarity scores\n",
    "    fpr, tpr, _ = roc_curve(all_labels, similarity_scores)\n",
    "    precision, recall, _ = precision_recall_curve(all_labels, similarity_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    pr_auc = auc(recall, precision)\n",
    "\n",
    "    # Calculate predictions using similarity threshold\n",
    "    predictions = (similarity_scores >= threshold).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    tp = np.sum((predictions == 1) & (all_labels == 1))\n",
    "    fp = np.sum((predictions == 1) & (all_labels == 0))\n",
    "    tn = np.sum((predictions == 0) & (all_labels == 0))\n",
    "    fn = np.sum((predictions == 0) & (all_labels == 1))\n",
    "    total = len(all_labels)\n",
    "\n",
    "    test_metrics = {\n",
    "        'accuracy': (tp + tn) / total,\n",
    "        'precision': tp / (tp + fp) if (tp + fp) > 0 else 0,\n",
    "        'recall': tp / (tp + fn) if (tp + fn) > 0 else 0,\n",
    "        'f1': 2 * (tp / (tp + fp)) * (tp / (tp + fn)) / ((tp / (tp + fp)) + (tp / (tp + fn))) if (tp + fp) > 0 and (tp + fn) > 0 else 0,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc\n",
    "    }\n",
    "\n",
    "    # Plot evaluation curves and distributions\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # ROC curve\n",
    "    plt.subplot(131)\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Test ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # Precision-Recall curve\n",
    "    plt.subplot(132)\n",
    "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AUC = {pr_auc:.2f})')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Test Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # Similarity distributions\n",
    "    plt.subplot(133)\n",
    "    plt.hist(similarity_scores[all_labels == 1], bins=50, alpha=0.5, label='Same Identity', density=True)\n",
    "    plt.hist(similarity_scores[all_labels == 0], bins=50, alpha=0.5, label='Different Identity', density=True)\n",
    "    plt.axvline(x=threshold, color='r', linestyle='--', label='Threshold')\n",
    "    plt.xlabel('Similarity Score')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Test Similarity Distributions')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nTest Results:\")\n",
    "    for metric, value in test_metrics.items():\n",
    "        print(f\"{metric}: {value:.3f}\")\n",
    "\n",
    "    return test_metrics"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example usage",
   "id": "2436e2c0cc0e84fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "v3 = load_model('../results/siamese/contrastive_v3/contrastive_v3.h5',compile=False)\n",
    "v3.compile()"
   ],
   "id": "2209de3ffa742f03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "test_metrics = evaluate_on_test(v3,test_contrastive_dataset)",
   "id": "3f7976b47fc49cfc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
